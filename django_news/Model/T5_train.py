import sys
import os
import torch
from transformers import AdamW
from rouge import Rouge
from tqdm import tqdm
from transformers import MT5ForConditionalGeneration, T5Tokenizer
from torch import nn
from transformers import Trainer, TrainingArguments
device = "cuda" if torch.cuda.is_available() else "cpu"
save_path="Model/"
model_heack = MT5ForConditionalGeneration.from_pretrained("heack/HeackMT5-ZhSum100k").to(device)
tokenizer_heack = T5Tokenizer.from_pretrained("heack/HeackMT5-ZhSum100k")



def T5_train(content,sum,b,gradient_accumulation_steps):
    inputs = tokenizer_heack.encode("summarize: " + content, return_tensors='pt', max_length=1024, truncation=True).to(device)
    # summary_ids = model_heack.generate(inputs, max_length=1024, num_beams=10, length_penalty=0.00001,
    #                                    no_repeat_ngram_size=10).to("cuda:0")
    labels=tokenizer_heack.encode(sum, return_tensors='pt', max_length=1024, truncation=True).to(device)
    outputs = model_heack(inputs, labels=labels)
    loss = outputs.loss
    optimizer = AdamW(model_heack.parameters(), lr=1e-4,no_deprecation_warning=True)
    # data1=model_heack.lm_head(outputs.last_hidden_state[:, 0, :])
    # print("content:"+content)
    # print("title:"+sum)
    # # print("summary:"+summary)
    # print(loss)
    # print("before:")
    # for _, param in enumerate(model_heack.named_parameters()):
    #     print(param[0])
    #     print(param[1])
    #     break
    # print(model_heack.lm_head.parameters())
    loss.backward()
    if b % gradient_accumulation_steps == 0:
        optimizer.step()  # åœ¨ç´¯ç§¯äº†è¶³å¤Ÿçš„æ¢¯åº¦ä¹‹åæ‰§è¡Œæ¢¯åº¦æ›´æ–°
        optimizer.zero_grad()
    return loss
    # print("after:")
    # for _, param in enumerate(model_heack.named_parameters()):
    #     print(param[0])
    #     print(param[1])
    #     break

def T5_eval():
    chunk="2021å¹´1æœˆï¼ŒCCFå†³å®šå¯åŠ¨æ–°ä¸€è½®ä¸­å›½è®¡ç®—æœºå­¦ä¼šæ¨èå›½é™…å­¦æœ¯ä¼šè®®å’ŒæœŸåˆŠç›®å½•ï¼ˆä»¥ä¸‹ç®€ç§°ã€Šç›®å½•ã€‹ï¼‰è°ƒæ•´å·¥ä½œå¹¶å§”æ‰˜CCFå­¦æœ¯å·¥ä½œå§”å‘˜ä¼šç»„ç»‡å®æ–½ã€‚ç»è¿‡å‰æœŸçš„å……åˆ†è®¨è®ºå’Œè®ºè¯åï¼Œäº2021å¹´9æœˆå¼€å§‹æ­£å¼å‘å„ä¸“å§”ä¼šå¾é›†è°ƒæ•´å»ºè®®ã€‚æœŸé—´ç”±äºç–«æƒ…åå¤ï¼Œæœ€ç»ˆç›®å½•è°ƒæ•´çš„è¯„å®¡ä¼šè®®äº2022å¹´10æœˆåœ¨åŒ—äº¬å¬å¼€ï¼Œä¼šè®®ä»¥çº¿ä¸Šçº¿ä¸‹ç›¸ç»“åˆçš„æ–¹å¼ï¼Œé¡ºåˆ©å®Œæˆæœ¬æ¬¡ç›®å½•çš„ä¿®è®¢å·¥ä½œã€‚æœ¬æ¬¡ç›®å½•è°ƒæ•´çš„æ€»ä½“åŸåˆ™æ˜¯ï¼šåœ¨æ—¢æœ‰åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä¿æŒå®ç¼ºæ¯‹æ»¥çš„åŸåˆ™ï¼›é¢†åŸŸåˆ’åˆ†æ–¹å¼ä¿æŒä¸å˜ï¼ŒæœŸåˆŠå’Œä¼šè®®çš„æ¨èç±»åˆ«ä½“ç³»ä¿æŒä¸å˜ï¼Œåœ¨åŒç­‰æƒ…å†µä¸‹å¢åŠ å¯¹å›½å†…æœŸåˆŠçš„æ”¯æŒã€‚æœ¬æ¬¡ç›®å½•è°ƒæ•´å·¥ä½œåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µå®Œæˆï¼šæè®®å—ç†é˜¶æ®µï¼Œé¢†åŸŸè´£ä»»ä¸“å®¶å®¡è®®å’Œåˆå®¡æ¨èé˜¶æ®µï¼Œä»¥åŠç»ˆå®¡æ ¸å‡†é˜¶æ®µã€‚æ ¹æ®CCFçš„æˆæƒå’Œå·¥ä½œå®‰æ’ï¼Œæ•´ä¸ªè°ƒæ•´å·¥ä½œç”±CCFå­¦æœ¯å·¥ä½œå§”å‘˜ä¼šä¸»æŒå¹¶ç»„ç»‡CCFç›¸å…³é¢†åŸŸçš„ä¸“å®¶å®Œæˆã€‚åŒæ—¶ï¼ŒCCFå­¦æœ¯å·¥ä½œå§”å‘˜ä¼šè¿˜è´Ÿè´£ä¸ºåˆå®¡æ¨èé˜¶æ®µæ”¶é›†ã€æ•´ç†å’Œæä¾›æ‰€éœ€è¦çš„æœŸåˆŠä¼šè®®ç›¸å…³æ•°æ®ä»¥åŠå›½é™…ä¸ŠåŒè¡Œçš„è§‚ç‚¹ä¸çœ‹æ³•å¹¶åˆ¶ä½œäº†åœ¨çº¿æŸ¥è¯¢ç³»ç»Ÿï¼Œä»¥åŠåˆ¶ä½œäº†ç»ˆå®¡ä¼šè®®çš„è¯¦ç»†ææ–™ã€‚"
    inputs = tokenizer_heack.encode("summarize: " + chunk, return_tensors='pt', max_length=1024, truncation=True)
    summary_ids = model_heack.generate(inputs, max_length=1024, num_beams=10, length_penalty=0.00001,
                                       no_repeat_ngram_size=10).to(device)
    summary = tokenizer_heack.decode(summary_ids[0], skip_special_tokens=True)
    criteria = rouge_loss
    # print(outputs.last_hidden_state.size())
    # print(model_heack.encoder.block[0].layer[0].SelfAttention)
    optimizer = AdamW(model_heack.parameters(), lr=1e-6,no_deprecation_warning=True)
    # data1=model_heack.lm_head(outputs.last_hidden_state[:, 0, :])
    loss = torch.tensor([criteria(summary, summary)], requires_grad=True)
    print(loss)

def rouge_loss(hypotheses, references, rouge_types=['rouge-1', 'rouge-2', 'rouge-l']):
    """
    è®¡ç®—ROUGEæŸå¤±å‡½æ•°

    Args:
        hypotheses (list of str): æ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦åˆ—è¡¨
        references (list of str): å‚è€ƒæ‘˜è¦åˆ—è¡¨
        rouge_types (list of str): ROUGEæŒ‡æ ‡çš„ç±»å‹ï¼Œé»˜è®¤åŒ…æ‹¬'rouge-1', 'rouge-2', 'rouge-l'
    Returns:
        loss (float): ROUGEæŸå¤±å€¼
    """
    rouge = Rouge()

    # ä½¿ç”¨ROUGEåº“è®¡ç®—ROUGEæŒ‡æ ‡
    scores = rouge.get_scores(hypotheses, references, avg=True, ignore_empty=True)

    # è®¡ç®—æŸå¤±å€¼ï¼ˆ1 - ROUGEå¾—åˆ†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„æŸå¤±å‡½æ•°ç¤ºä¾‹
    loss = 0.0
    for rouge_type in rouge_types:
        loss += 1.0 - scores[rouge_type]['f']

    # è¿”å›å¹³å‡æŸå¤±å€¼
    return loss / len(rouge_types)

if __name__ == '__main__':
    dataset=get_dataset()
    # training_args = TrainingArguments(
    #     output_dir='./results',  # output directory ç»“æœè¾“å‡ºåœ°å€
    #     num_train_epochs=1000,  # total # of training epochs è®­ç»ƒæ€»æ‰¹æ¬¡
    #     per_device_train_batch_size=128,  # batch size per device during training è®­ç»ƒæ‰¹å¤§å°
    #     per_device_eval_batch_size=128,  # batch size for evaluation è¯„ä¼°æ‰¹å¤§å°
    #     logging_dir='./logs/rn_log',  # directory for storing logs æ—¥å¿—å­˜å‚¨ä½ç½®
    #     learning_rate=1e-6,  # å­¦ä¹ ç‡
    #     save_steps=False,  # ä¸ä¿å­˜æ£€æŸ¥ç‚¹
    #     remove_unused_columns=False
    # )
    #
    # trainer = Trainer(
    #     model=model_heack,  # the instantiated ğŸ¤— Transformers model to be trained éœ€è¦è®­ç»ƒçš„æ¨¡å‹
    #     args=training_args,  # training arguments, defined above è®­ç»ƒå‚æ•°
    #     train_dataset=dataset,  # training dataset è®­ç»ƒé›†
    #     eval_dataset=dataset,  # evaluation dataset æµ‹è¯•é›†
    #     compute_metrics=rouge_loss
    # )
    #
    # trainer.train()
    # # trainer.evaluate()
    # trainer.save_model(output_dir="./Model")
    # k=0
    # for i in dataset:
    #     if k==0:
    #         T5_train(i["content"],i["title"])
    #         break

    #æŒ‰ç…§è‡ªå·±çš„æ–¹æ³•trainä¼šå¯¼è‡´æ¨¡å‹è¯»ä¸äº†
    epoches=1000
    batch_size=128
    e=1
    gradient_accumulation_steps = 4  # æ¢¯åº¦ç´¯ç§¯çš„æ­¥æ•°
    total_iterations = epoches * batch_size// gradient_accumulation_steps
    # åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡å¯¹è±¡
    progress_bar = tqdm(total=total_iterations, desc="Processing")
    min_loss = sys.maxsize

    # å‡è®¾æ‚¨å·²ç»å°†æ•°æ®å­˜å‚¨åœ¨åä¸º "dataset" çš„ Pandas DataFrame ä¸­
    # æ‚¨å¯ä»¥ä½¿ç”¨ Pandas çš„ sample å‡½æ•°éšæœºæŠ½æ ·
    random_sample = dataset.sample(n=batch_size, random_state=42)
    for i in range(epoches):
        b=1
        for index, row in random_sample.iterrows():
            loss = T5_train(row["text"], row["target"],b,gradient_accumulation_steps)
            progress_bar.update(1)
            progress_bar.set_postfix(loss=loss)

            progress_bar.update(1)
            if loss < min_loss:
                modelpath = os.path.join(save_path)
                model_heack.save_pretrained(modelpath)
                min_loss=loss
            b=b+1




